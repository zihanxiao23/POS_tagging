{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely state sequence: ['DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.', 'DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NUM', 'NOUN', '.', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n",
      "Probability of this sequence: 1.015640858338443e-149\n",
      "True tags: ['DET', 'VERB', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.', 'DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NUM', 'DET', '.', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Ensure the Brown corpus and Universal Tagset are downloaded\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# Extract the first 10,000 tagged sentences\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')[:10000]\n",
    "\n",
    "# Create mappings for states and observations\n",
    "states = set()\n",
    "observations = set()\n",
    "for sentence in tagged_sentences:\n",
    "    for word, tag in sentence:\n",
    "        states.add(tag)\n",
    "        observations.add(word)\n",
    "\n",
    "# Add UNK observation\n",
    "observations.add(\"UNK\")\n",
    "\n",
    "# Map states and observations to indices\n",
    "state_to_index = {state: i for i, state in enumerate(states)}\n",
    "observation_to_index = {obs: i for i, obs in enumerate(observations)}\n",
    "\n",
    "# HMM components\n",
    "num_states = len(states)\n",
    "num_observations = len(observations)\n",
    "\n",
    "# Initialize transition matrix, observation matrix, and initial state distribution\n",
    "A = np.ones((num_states, num_states))  # State transition matrix (add-1 smoothing)\n",
    "B = np.ones((num_states, num_observations))  # Observation matrix (add-1 smoothing)\n",
    "pi = np.ones(num_states)  # Initial state probabilities (add-1 smoothing)\n",
    "\n",
    "# Fill in the state transition matrix, observation matrix, and initial state distribution\n",
    "state_counts = Counter()\n",
    "for sentence in tagged_sentences:\n",
    "    previous_state = None\n",
    "    for word, tag in sentence:\n",
    "        state_index = state_to_index[tag]\n",
    "        observation_index = observation_to_index.get(word, observation_to_index[\"UNK\"])\n",
    "\n",
    "        # Update initial state probabilities\n",
    "        if previous_state is None:\n",
    "            pi[state_index] += 1\n",
    "        \n",
    "        # Update state transition matrix\n",
    "        if previous_state is not None:\n",
    "            A[state_to_index[previous_state], state_index] += 1\n",
    "        \n",
    "        # Update observation matrix\n",
    "        B[state_index, observation_index] += 1\n",
    "\n",
    "        # Update state counts\n",
    "        state_counts[tag] += 1\n",
    "        previous_state = tag\n",
    "\n",
    "# Normalize the state transition matrix and observation matrix\n",
    "for i in range(num_states):\n",
    "    A[i] /= np.sum(A[i])  # Normalize rows\n",
    "    B[i] /= np.sum(B[i])  # Normalize rows\n",
    "\n",
    "# Normalize initial state probabilities\n",
    "pi /= np.sum(pi)\n",
    "\n",
    "# Use the Viterbi algorithm to infer the state sequence for sentences 10150 to 10152\n",
    "test_sentences = brown.tagged_sents(tagset='universal')[10150:10153]\n",
    "\n",
    "# Extract observation sequence and convert to indices\n",
    "test_obs = []\n",
    "for sentence in test_sentences:\n",
    "    for word, _ in sentence:\n",
    "        # If the word is not in the observation mapping, use the index for UNK\n",
    "        test_obs.append(observation_to_index.get(word, observation_to_index[\"UNK\"]))\n",
    "\n",
    "# Call the Viterbi function\n",
    "from viterbi import viterbi  # Ensure viterbi.py is in the same directory\n",
    "\n",
    "most_likely_states, probability = viterbi(test_obs, pi, A, B)\n",
    "\n",
    "# Print results\n",
    "print(\"Most likely state sequence:\", [list(states)[state] for state in most_likely_states])\n",
    "print(\"Probability of this sequence:\", probability)\n",
    "\n",
    "# Compare inferred results with true tags\n",
    "true_tags = [tag for sentence in test_sentences for _, tag in sentence]\n",
    "print(\"True tags:\", true_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
